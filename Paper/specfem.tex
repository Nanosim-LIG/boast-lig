\newcommand{\productname}[1]{\textsc{#1}}
\newcommand{\latin}[1]{\textit{#1}}

\newcommand{\Specfem}{\productname{Specfem 3D}\xspace}
\newcommand{\Cuda}{\productname{Cuda}\xspace}
\newcommand{\eg}{\latin{e.g.,}\xspace}
\newcommand{\OCL}{\productname{OpenCL}\xspace}
\newcommand{\code}[1]{\texttt{#1}}

\newcommand{\etc}[1]{\latin{etc}}

\section{BOAST Use Cases}

\subsection{Porting SPECFEM3D to OpenCL using BOAST}

\subsubsection{SPECFEM3D}

In this last subsection, we study a free seismic wave propagation
simulator, \Specfem, in its \productname{Cartesian}
flavor\footnote{Specfem3d Cartesian --- CIG
  \url{http://www.geodynamics.org/cig/software/specfem3d}}. \Specfem
simulates seismic wave propagation at the local or regional scale
based upon spectral-element method (SEM), with very good accuracy and
convergence properties. ADD BENCHMARK APP FOR HPC SUPERCOMPUTERS.

When we started to work on the project (version v2.1 of July 2013) it
supported graphics card GPU acceleration through NVidia \Cuda. This
GPU support comes in addition to the MPI support implemented to enable
parallel computing. Most of \Specfem code-base is written in
\productname{Fortran2003}, only the GPU related parts are written in
C.  The split between CPU and GPU code was done at a rather fine
grain, as the application counted more than 40 GPU kernels. Some of
them were quite simple (\eg performing a few vector operations, but at
massively parallel scale), while at the other side of the spectrum,
some complex kernels took more than 80 parameters and perform very
specific physical transformations.

Because of the complexity of wave propagation (and of the application
architecture), it is hard to impossible to define unit tests. Hence,
the application is validated by the accuracy of the results it
produces, that is, the accuracy of seismograms of the simulated
earthquakes, in comparison with the actual ones. Non-regression
testing (after new developments) is also based on these seismograms,
with measurements of the relative error between two identical
simulations.

As part of the Mont-Blanc project, we had to port \Specfem to \OCL, so
that it could be use to benchmark Mont-Blanc HPC platforms.

\subsubsection{Porting to OpenCL}

\subparagraph{Porting kernels to BOAST} Nvidia \Cuda and \OCL are
based on the \emph{same} programming model: a massively parallel
accelerator running in a distinct memory environment. Thanks to that
proximity, we, BOAST developers, have been able to carry out most of
the porting task with only a limited knowledge about \Specfem internal
physics.

That lack of knowledge led us to be particularly attentive to the path
we undertook for the porting, as we would have been unable to
understand how and why the application was not operating properly, if
it was to fail.

Hence, our first milestone in the porting was translating \Specfem
\Cuda kernels to BOAST EDSL. This way, we could ask BOAST framework to
generate CUDA versions of the kernels, plug them back into \Specfem and
get (after fixing compile-time errors---prototypes and naming mistakes
mainly) a first set of \Specfem seismograms.

As we had expected, the seismograms were erroneous. But with the help
of shell scripts and BOAST framework ability to store and provide the
kernels' original source code, we built a set of \Specfem binaries
including only \emph{one} BOAST-generated kernel, with all others
reference-kernels. Running and validating all these binaries enabled
use to pinpoint the misbehaving kernels. We finished the debugging
with a side-by-side comparison that highlighted the coding mistakes.

\subparagraph{Porting runtime to \OCL} The second part of the porting
consisted in translating the CPU-side of the application from \Cuda
API to \OCL API. Most of the functions of the interfaces are very
similar, with only naming-convention and data-structure
distinctions. Hence, it was clear that automatic rewriting tools
(namely \code{sed} regexp and \code{emacs-list} functions) could be
useful. To give an idea of the cost of a \emph{manual} rewriting, we
can count (in \# of \OCL API function calls): 70 kernel ``function
calls'', 790 arguments to set, 230 memory transfers, 160 buffer
creations and 270 releases.

Once all the automatic (and many manual) transformations applied,
compilation errors fixed, and \OCL unsuccessful function calls solved,
the application managed to complete its execution and generated a
first set of seismograms. And again, as expected and feared, these
seismograms were not valid.

As we had already validated BOAST-generated kernels (and trusted \Cuda
and \OCL versions to be semantically identical), we knew that the bugs
were now in the CPU side, and we had to find a way to understand where
\Specfem's \Cuda version of the code diverged from its \OCL
counterpart. To help us in that purpose, we had a strong assumption:
both versions of the code were supposed to perform exactly the same
operations, with the same ``logical'' parameters (the APIs have
\emph{implementation} differences, for instance \OCL has two memory
transfer functions, \code{clEnqueueReadBuffer},
\code{clEnqueueWriteBuffer}, whereas \Cuda has only one, with a
direction parameter \code{cudaMemcpy(..., dir)}, but above that it is
the same functionality nonetheless).

Hence, our idea to locate the execution problems was to make sure that
both execution actually did the same thing. As the \OCL results were
invalid, we knew the executions would diverge at one or several
points.

\subparagraph{Debugging \OCL Execution: \code{GPUTrace}} In order to
ensure that \Cuda and \OCL executions were doing the same operations,
we developed a tool that could intercept all the function calls
related to the GPU and store them for offline processing, in a format
common to \OCL and \Cuda. Our solution was inspired by \code{strace}
and \code{ltrace} tools: it dynamically preloads a library (with the
help \code{LD\_PRELOAD} in \productname{GNU/Linux}-based environments)
between the application and the GPU library, and prints the arguments
(input \emph{and} output) of the relevant function calls.

However, in contrast with \code{strace} and \code{ltrace},
\code{GPUTrace} has to be \emph{state-full}. Indeed, most of \OCL and
\Cuda parameters are handles to opaque types, so, in order to provide
more meaningful information, \code{GPUTrace} stores internally
information about these handles (buffer creation size and index,
kernel name and prototype, \etc). % more details here?

Once \code{GPUTrace} was operational, it was easy to confront \Cuda
and \OCL outputs with a graphical \code{diff} tool, and spot the
different porting mistakes: two parameters reversed, an offset
incorrectly applied, \etc. One last problem remained, impossible to
miss because of the seismograms not matching (but with just a slight
drift). We added more verbosity to \code{GPUTrace} output, first the
initial bits of the GPU memory buffers, then their full content. The
drift was visible in the trace, but it was nonetheless unclear where
it started. We finally got it after hours of code review, both on
BOAST kernels and \OCL code: one kernel was \emph{three}-dimensional,
whereas the others were two-dimensional. But for all of them, only two
dimensions were passed. As it was the same person who coded the \OCL
port and the GPU tracer, this third dimension was forgotten in both
side\ldots

\subparagraph{Evaluation} Our \OCL/BOAST port of \Specfem is now
merged in \Specfem's development tree and under test and extension by
different research teams. On a platform with two K40x GPUs and x Intel
Xeon processors, we measured similar results between the original
\Cuda version and our BOAST-\Cuda version. With the same set of
optimization, BOAST \Cuda and \OCL version also reported similar
execution time spans. The best execution speed (25\% higher) was
achieved with \Cuda version though, as one optimization parameter
(\code{CUDA\_LAUNCH\_BOUND}) cannot be passed, as of version 1.1, to
\OCL runtime.

By refactoring GPU kernel code in BOAST EDSL, the size of kernel code
shrank by a factor of 1.5 (from xxx LOC to xxx, in part because of
code duplication, but by removing manually unrolled loops). This is
beneficial for \Specfem as it improves the readability and
maintainability of its source code.

We have also been able to enhance \Specfem's non-regression testsuite
by adding per-kernel non-regression tests. This was done with the help
of \code{GPUTrace}, that we used to capture all the input parameters
of a particular \emph{valid} kernel execution, as well as the output
values. Then, during the non-regression testing, BOAST framework can
load these buffer files, allocate and write them on the GPU memory
through \Cuda or \OCL runtime, and trigger the kernel execution. A
comparison of the output values (for instance against a maximal error
level) validates the non-regression.

In the same mindset, we provided \Specfem testsuite with kernel
performance evaluation mechanisms. These tests will help developers to
try new optimization in kernel code and measure their impact, without
executing the whole application.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% Local Variables:      %%%
%%% mode: latex           %%%
%%% TeX-master: "journal" %%%
%%% TeX-PDF-mode: t       %%%
%%% End:                  %%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
